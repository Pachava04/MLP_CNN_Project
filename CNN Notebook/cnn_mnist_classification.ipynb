{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f1a7500f",
      "metadata": {
        "id": "f1a7500f"
      },
      "source": [
        "\n",
        "#Build, Train, and Test CNN on MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eca274a",
      "metadata": {
        "id": "0eca274a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a7b3463",
      "metadata": {
        "id": "1a7b3463"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "886813ce",
      "metadata": {
        "id": "886813ce",
        "outputId": "8540bc9f-1b85-4b4b-e41f-ef6d127dc216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "ON STEP: 0\n",
            "ACCURACY:\n",
            "0.1033\n",
            "\n",
            "\n",
            "ON STEP: 100\n",
            "ACCURACY:\n",
            "0.9395\n",
            "\n",
            "\n",
            "ON STEP: 200\n",
            "ACCURACY:\n",
            "0.9593\n",
            "\n",
            "\n",
            "ON STEP: 300\n",
            "ACCURACY:\n",
            "0.9712\n",
            "\n",
            "\n",
            "ON STEP: 400\n",
            "ACCURACY:\n",
            "0.973\n",
            "\n",
            "\n",
            "ON STEP: 500\n",
            "ACCURACY:\n",
            "0.9758\n",
            "\n",
            "\n",
            "ON STEP: 600\n",
            "ACCURACY:\n",
            "0.9779\n",
            "\n",
            "\n",
            "ON STEP: 700\n",
            "ACCURACY:\n",
            "0.9744\n",
            "\n",
            "\n",
            "ON STEP: 800\n",
            "ACCURACY:\n",
            "0.9817\n",
            "\n",
            "\n",
            "ON STEP: 900\n",
            "ACCURACY:\n",
            "0.9824\n",
            "\n",
            "\n",
            "ON STEP: 1000\n",
            "ACCURACY:\n",
            "0.9835\n",
            "\n",
            "\n",
            "ON STEP: 1100\n",
            "ACCURACY:\n",
            "0.9847\n",
            "\n",
            "\n",
            "ON STEP: 1200\n",
            "ACCURACY:\n",
            "0.9862\n",
            "\n",
            "\n",
            "ON STEP: 1300\n",
            "ACCURACY:\n",
            "0.9852\n",
            "\n",
            "\n",
            "ON STEP: 1400\n",
            "ACCURACY:\n",
            "0.9844\n",
            "\n",
            "\n",
            "ON STEP: 1500\n",
            "ACCURACY:\n",
            "0.9859\n",
            "\n",
            "\n",
            "ON STEP: 1600\n",
            "ACCURACY:\n",
            "0.9849\n",
            "\n",
            "\n",
            "ON STEP: 1700\n",
            "ACCURACY:\n",
            "0.9864\n",
            "\n",
            "\n",
            "ON STEP: 1800\n",
            "ACCURACY:\n",
            "0.9844\n",
            "\n",
            "\n",
            "ON STEP: 1900\n",
            "ACCURACY:\n",
            "0.9868\n",
            "\n",
            "\n",
            "ON STEP: 2000\n",
            "ACCURACY:\n",
            "0.9867\n",
            "\n",
            "\n",
            "ON STEP: 2100\n",
            "ACCURACY:\n",
            "0.9846\n",
            "\n",
            "\n",
            "ON STEP: 2200\n",
            "ACCURACY:\n",
            "0.9867\n",
            "\n",
            "\n",
            "ON STEP: 2300\n",
            "ACCURACY:\n",
            "0.9878\n",
            "\n",
            "\n",
            "ON STEP: 2400\n",
            "ACCURACY:\n",
            "0.9896\n",
            "\n",
            "\n",
            "ON STEP: 2500\n",
            "ACCURACY:\n",
            "0.9894\n",
            "\n",
            "\n",
            "ON STEP: 2600\n",
            "ACCURACY:\n",
            "0.987\n",
            "\n",
            "\n",
            "ON STEP: 2700\n",
            "ACCURACY:\n",
            "0.9897\n",
            "\n",
            "\n",
            "ON STEP: 2800\n",
            "ACCURACY:\n",
            "0.9889\n",
            "\n",
            "\n",
            "ON STEP: 2900\n",
            "ACCURACY:\n",
            "0.9897\n",
            "\n",
            "\n",
            "ON STEP: 3000\n",
            "ACCURACY:\n",
            "0.99\n",
            "\n",
            "\n",
            "ON STEP: 3100\n",
            "ACCURACY:\n",
            "0.9899\n",
            "\n",
            "\n",
            "ON STEP: 3200\n",
            "ACCURACY:\n",
            "0.99\n",
            "\n",
            "\n",
            "ON STEP: 3300\n",
            "ACCURACY:\n",
            "0.9916\n",
            "\n",
            "\n",
            "ON STEP: 3400\n",
            "ACCURACY:\n",
            "0.9906\n",
            "\n",
            "\n",
            "ON STEP: 3500\n",
            "ACCURACY:\n",
            "0.9895\n",
            "\n",
            "\n",
            "ON STEP: 3600\n",
            "ACCURACY:\n",
            "0.99\n",
            "\n",
            "\n",
            "ON STEP: 3700\n",
            "ACCURACY:\n",
            "0.9905\n",
            "\n",
            "\n",
            "ON STEP: 3800\n",
            "ACCURACY:\n",
            "0.9892\n",
            "\n",
            "\n",
            "ON STEP: 3900\n",
            "ACCURACY:\n",
            "0.987\n",
            "\n",
            "\n",
            "ON STEP: 4000\n",
            "ACCURACY:\n",
            "0.9892\n",
            "\n",
            "\n",
            "ON STEP: 4100\n",
            "ACCURACY:\n",
            "0.9923\n",
            "\n",
            "\n",
            "ON STEP: 4200\n",
            "ACCURACY:\n",
            "0.99\n",
            "\n",
            "\n",
            "ON STEP: 4300\n",
            "ACCURACY:\n",
            "0.9917\n",
            "\n",
            "\n",
            "ON STEP: 4400\n",
            "ACCURACY:\n",
            "0.9856\n",
            "\n",
            "\n",
            "ON STEP: 4500\n",
            "ACCURACY:\n",
            "0.992\n",
            "\n",
            "\n",
            "ON STEP: 4600\n",
            "ACCURACY:\n",
            "0.9882\n",
            "\n",
            "\n",
            "ON STEP: 4700\n",
            "ACCURACY:\n",
            "0.9897\n",
            "\n",
            "\n",
            "ON STEP: 4800\n",
            "ACCURACY:\n",
            "0.992\n",
            "\n",
            "\n",
            "ON STEP: 4900\n",
            "ACCURACY:\n",
            "0.9861\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "y_true = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "hold_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
        "\n",
        "def initialize_weights(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def initialize_bias(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def create_convolution_layer_and_compute_dot_product(inputs, filter_shape):\n",
        "    filter_initialized_with_weights = initialize_weights(filter_shape)\n",
        "    conv_layer_output = tf.nn.conv2d(inputs, filter_initialized_with_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
        "    return conv_layer_output\n",
        "\n",
        "def create_relu_layer_and_compute_dotproduct_plus_b(inputs, filter_shape):\n",
        "    b = initialize_bias([filter_shape[3]])\n",
        "    relu_layer_output = tf.nn.relu(inputs + b)\n",
        "    return relu_layer_output\n",
        "\n",
        "def create_maxpool2by2_and_reduce_spatial_size(inputs):\n",
        "    pooling_layer_output = tf.nn.max_pool(inputs, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "    return pooling_layer_output\n",
        "\n",
        "def create_fully_connected_layer_and_compute_dotproduct_plus_bias(input, output_size):\n",
        "    weights = initialize_weights([input.get_shape()[1], output_size])\n",
        "    bias = initialize_bias([output_size])\n",
        "    layer = tf.matmul(input, weights) + bias\n",
        "    return layer\n",
        "\n",
        "conv_layer_1_outputs = create_convolution_layer_and_compute_dot_product(x_image, filter_shape=[5, 5, 1, 32])\n",
        "conv_relu_layer_1_outputs = create_relu_layer_and_compute_dotproduct_plus_b(conv_layer_1_outputs, filter_shape=[5, 5, 1, 32])\n",
        "pooling_layer_1_outputs = create_maxpool2by2_and_reduce_spatial_size(conv_relu_layer_1_outputs)\n",
        "\n",
        "conv_layer_2_outputs = create_convolution_layer_and_compute_dot_product(pooling_layer_1_outputs, filter_shape=[5, 5, 32, 64])\n",
        "conv_relu_layer_2_outputs = create_relu_layer_and_compute_dotproduct_plus_b(conv_layer_2_outputs, filter_shape=[5, 5, 32, 64])\n",
        "pooling_layer_2_outputs = create_maxpool2by2_and_reduce_spatial_size(conv_relu_layer_2_outputs)\n",
        "pooling_layer_2_outputs_flat = tf.reshape(pooling_layer_2_outputs, [-1, 7*7*64])\n",
        "\n",
        "\n",
        "fc_layer_1_outputs = create_fully_connected_layer_and_compute_dotproduct_plus_bias(pooling_layer_2_outputs_flat, output_size=1024)\n",
        "fc_relu_layer_1_outputs = tf.nn.relu(fc_layer_1_outputs)\n",
        "\n",
        "fc_dropout_outputs = tf.nn.dropout(fc_relu_layer_1_outputs, keep_prob=hold_prob)\n",
        "\n",
        "\n",
        "y_pred = create_fully_connected_layer_and_compute_dotproduct_plus_bias(fc_dropout_outputs, output_size=10)\n",
        "\n",
        "softmax_cross_entropy_loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
        "cross_entropy_mean = tf.reduce_mean(softmax_cross_entropy_loss)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
        "cnn_trainer = optimizer.minimize(cross_entropy_mean)\n",
        "\n",
        "vars_initializer = tf.global_variables_initializer()\n",
        "\n",
        "steps = 5000\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(vars_initializer)\n",
        "\n",
        "    for i in range(steps):\n",
        "        batch_x, batch_y = mnist.train.next_batch(50)\n",
        "        sess.run(cnn_trainer, feed_dict={x: batch_x, y_true: batch_y, hold_prob: 0.5})\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('ON STEP: {}'.format(i))\n",
        "            print('ACCURACY:')\n",
        "            matches = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
        "            acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
        "            test_accuracy = sess.run(acc, feed_dict={x: mnist.test.images, y_true: mnist.test.labels, hold_prob: 1.0})\n",
        "            print(test_accuracy)\n",
        "            print('\\n')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}